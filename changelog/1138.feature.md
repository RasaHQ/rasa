Create a self-hosted LLM client compatible with OpenAI format.
Users can connect to their own self-hosted LLM server that is compatible with OpenAI format.

Sample basic usage:
```
    llm:
        provider: self-hosted
        model: <deployment_name>
        api_base: <deployment_url>
        api_type: openai [Optional]
```