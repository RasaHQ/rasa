
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Components</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/banner.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="The Rasa Core Dialogue Engine" href="../../core/about/" />
    <link rel="prev" title="Entity Extraction" href="../entity-extraction/" />

  <!-- Google Tag Manager -->
  <script type="opt-in" data-type="application/javascript" data-name="analytics">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MMHSZCS');</script>
  <!-- End Google Tag Manager -->
   
  
  <meta itemprop="image" content="https://rasa.com/assets/img/facebook-og.png">
  <meta property="og:title" content="Components" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://rasa.com/assets/img/facebook-og.png" />
  <meta property="og:url" content="https://rasa.com/docs/rasa/nlu/components" />
  
    <meta name="description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline" />
    <meta itemprop="description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline">
    <meta name="twitter:description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline" />
    <meta property="og:description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline" />
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@Rasa_HQ">
  <meta name="twitter:title" content="Components">
  <meta name="twitter:creator" content="@Rasa_HQ">
  <meta name="twitter:image" content="https://rasa.com/assets/img/facebook-og.png">

  <link rel="stylesheet" href="../../_static/xq-light.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/fontawesome/css/fontawesome-all.css" type="text/css" />
  <link rel="stylesheet" type="text/css" href="https://rasa.com/assets/css/klaro.css">
  <script defer type="text/javascript" src="https://rasa.com/assets/js/klaro_config.js"></script>
  <script defer type="text/javascript" src="https://rasa.com/assets/js/klaro.js"></script>
  <script defer type="text/javascript" src="../../_static/ace/src-min-noconflict/ace.js"></script>
  <script defer type="text/javascript" src="../../_static/chatblock/rasa-chatblock.min.js"></script>
  <script type="text/javascript" src="https://storage.googleapis.com/docs-theme/clipboard.min.js"></script>
  
    <link rel="icon" sizes="192x192" href="../../_static/icon-192x192.png">
    <link rel="apple-touch-icon" href="../../_static/icon-192x192.png" />
  
  


  </head><body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe data-name="analytics" data-src="https://www.googletagmanager.com/ns.html?id=GTM-MMHSZCS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<div class="announce-bar" role="banner">
  These docs are for version 1.x of Rasa Open Source.
  <a href="https://rasa.com/docs/rasa/">Docs for the new version 2.0 can be found here.</a>
</div>

<div class="nav-top">
  <div class="nav-container">
    <ul class="main-nav nav">
      <li>
      <a href="https://rasa.com/docs/" class="brand-link">
          <img src="../../_static/rasa_logo.svg" width="80px" height="40px" title="Rasa logo" alt="Rasa logo">
    	    <span class="logo extension">docs</span>
    	</a>
      </li>
    </ul>
    <ul class="secondary-nav nav">
      <li>
        <a href="https://github.com/rasaHQ/" target="_blank"><button class="button btn-ghost white"> <i class="fab fa-github"></i>GitHub</button></a>
      </li>
      <li>
        <a href="https://forum.rasa.com" target="_blank"><button class="button"><i class="fas fa-comments"></i> Ask the Community</button></a>
      </li>
    </ul>
  </div>
</div>

  
    
      <div class="sidebar-extended"></div>
    
  

  
    
      <div class="document">
    
  

    
      
        
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/installation/">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/rasa-tutorial/">Tutorial: Rasa Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/building-assistants/">Tutorial: Building Assistants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/command-line-interface/">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/architecture/">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/messaging-and-voice-channels/">Messaging and Voice Channels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/evaluating-models/">Evaluating Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/validate-files/">Validate Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/running-the-server/">Running the Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/running-rasa-with-docker/">Running Rasa with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/cloud-storage/">Cloud Storage</a></li>
</ul>
<p class="caption"><span class="caption-text">NLU</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../about/">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-nlu-only/">Using NLU Only</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training-data-format/">Training Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../choosing-a-pipeline/">Choosing a Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language-support/">Language Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../entity-extraction/">Entity Extraction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Components</a></li>
</ul>
<p class="caption"><span class="caption-text">Core</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../core/about/">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/stories/">Stories</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/domains/">Domains</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/responses/">Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/actions/">Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/policies/">Policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/slots/">Slots</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/forms/">Forms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/retrieval-actions/">Retrieval Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/interactive-learning/">Interactive Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/fallback-actions/">Fallback Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/knowledge-bases/">Knowledge Base Actions</a></li>
</ul>
<p class="caption"><span class="caption-text">Conversation Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/dialogue-elements/">Dialogue Elements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/small-talk/">Small Talk</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/completing-tasks/">Completing Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/guiding-users/">Guiding Users</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/action-server/">Action Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/http-api/">HTTP API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/jupyter-notebooks/">Jupyter Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/agent/">Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/custom-nlu-components/">Custom NLU Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/rasa-sdk/">Rasa SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/events/">Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/tracker/">Tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/tracker-stores/">Tracker Stores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/event-brokers/">Event Brokers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/lock-stores/">Lock Stores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/training-data-importers/">Training Data Importers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/core-featurization/">Featurization of Conversations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration-guide/">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog/">Rasa OSS Change Log</a></li>
</ul>
<p class="caption"><span class="caption-text">Migrate from (beta)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/google-dialogflow-to-rasa/">Dialogflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/facebook-wit-ai-to-rasa/">Wit.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/microsoft-luis-to-rasa/">LUIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/ibm-watson-to-rasa/">IBM Watson</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Glossary</a></li>
</ul>

<div class="versions">
    <p class="caption">Versions</p>
    <div class="versions-content">
      <div>
        <span class="current-version">
          viewing: 1.7.0
        </span>
      </div>
      <div class="other-versions">
          <p>tags</p>
          <div class="dropdown-content">
              <a href="../../../1.10.18/nlu/components/">1.10.18</a>
              <a href="../../../1.10.17/nlu/components/">1.10.17</a>
              <a href="../../../1.10.16/nlu/components/">1.10.16</a>
              <a href="../../../1.10.15/nlu/components/">1.10.15</a>
              <a href="../../../1.10.14/nlu/components/">1.10.14</a>
              <a href="../../../1.10.13/nlu/components/">1.10.13</a>
              <a href="../../../1.10.12/nlu/components/">1.10.12</a>
              <a href="../../../1.10.11/nlu/components/">1.10.11</a>
              <a href="../../../1.10.10/nlu/components/">1.10.10</a>
              <a href="../../../1.10.9/nlu/components/">1.10.9</a>
              <a href="../../../1.10.8/nlu/components/">1.10.8</a>
              <a href="../../../1.10.7/nlu/components/">1.10.7</a>
              <a href="../../../1.10.6/nlu/components/">1.10.6</a>
              <a href="../../../1.10.5/nlu/components/">1.10.5</a>
              <a href="../../../1.10.4/nlu/components/">1.10.4</a>
              <a href="../../../1.10.3/nlu/components/">1.10.3</a>
              <a href="../../../1.10.2/nlu/components/">1.10.2</a>
              <a href="../../../1.10.1/nlu/components/">1.10.1</a>
              <a href="../../../1.10.0/nlu/components/">1.10.0</a>
              <a href="../../../1.9.7/nlu/components/">1.9.7</a>
              <a href="../../../1.9.6/nlu/components/">1.9.6</a>
              <a href="../../../1.9.5/nlu/components/">1.9.5</a>
              <a href="../../../1.9.4/nlu/components/">1.9.4</a>
              <a href="../../../1.9.3/nlu/components/">1.9.3</a>
              <a href="../../../1.9.2/nlu/components/">1.9.2</a>
              <a href="../../../1.9.1/nlu/components/">1.9.1</a>
              <a href="../../../1.9.0/nlu/components/">1.9.0</a>
              <a href="../../../1.8.3/nlu/components/">1.8.3</a>
              <a href="../../../1.8.2/nlu/components/">1.8.2</a>
              <a href="../../../1.8.1/nlu/components/">1.8.1</a>
              <a href="../../../1.8.0/nlu/components/">1.8.0</a>
              <a href="../../../1.7.4/nlu/components/">1.7.4</a>
              <a href="../../../1.7.3/nlu/components/">1.7.3</a>
              <a href="../../../1.7.2/nlu/components/">1.7.2</a>
              <a href="../../../1.7.1/nlu/components/">1.7.1</a>
              <a href="../../../1.7.0/nlu/components/">1.7.0</a>
              <a href="../../../1.6.2/nlu/components/">1.6.2</a>
              <a href="../../../1.5.3/nlu/components/">1.5.3</a>
              <a href="../../../1.4.6/nlu/components/">1.4.6</a>
              <a href="../../../1.3.10/nlu/components/">1.3.10</a>
              <a href="../../../1.2.9/nlu/components/">1.2.9</a>
          </div>
      </div>
    </div>
</div>


        </div>
      </div>
      
    
      
        
      
        <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  


    
    



    <p class="scv-banner"><a href="../../../1.10.18/nlu/components/"><b>Warning:</b> This document is for an old version of Rasa. The latest version is 1.10.18.</a></p>
<div class="section" id="components">
<span id="id1"></span><h1>Components<a class="headerlink" href="#components" title="Permalink to this headline">¶</a></h1>

  <div class="edit-link">
    <a class="reference external" href="https://github.com/RasaHQ/rasa/edit/master/docs/nlu/components.rst" target="_blank"><i class="fab fa-github" style="font-size: 85%; padding-right: 4px;"></i>SUGGEST EDITS</a>
  </div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>For clarity, we have renamed the pre-defined pipelines to reflect
what they <em>do</em> rather than which libraries they use as of Rasa NLU
0.15. The <code class="docutils literal notranslate"><span class="pre">tensorflow_embedding</span></code> pipeline is now called
<code class="docutils literal notranslate"><span class="pre">supervised_embeddings</span></code>, and <code class="docutils literal notranslate"><span class="pre">spacy_sklearn</span></code> is now known as
<code class="docutils literal notranslate"><span class="pre">pretrained_embeddings_spacy</span></code>. Please update your code if you are using these.</p>
</div>
<p>This is a reference of the configuration options for every built-in
component in Rasa NLU. If you want to build a custom component, check
out <a class="reference internal" href="../../api/custom-nlu-components/#custom-nlu-components"><span class="std std-ref">Custom NLU Components</span></a>.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#word-vector-sources" id="id9">Word Vector Sources</a></p>
<ul>
<li><p><a class="reference internal" href="#mitienlp" id="id10">MitieNLP</a></p></li>
<li><p><a class="reference internal" href="#spacynlp" id="id11">SpacyNLP</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#text-featurizers" id="id12">Text Featurizers</a></p>
<ul>
<li><p><a class="reference internal" href="#mitiefeaturizer" id="id13">MitieFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#spacyfeaturizer" id="id14">SpacyFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#convertfeaturizer" id="id15">ConveRTFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#regexfeaturizer" id="id16">RegexFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#countvectorsfeaturizer" id="id17">CountVectorsFeaturizer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#intent-classifiers" id="id18">Intent Classifiers</a></p>
<ul>
<li><p><a class="reference internal" href="#mitieintentclassifier" id="id19">MitieIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#sklearnintentclassifier" id="id20">SklearnIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#embeddingintentclassifier" id="id21">EmbeddingIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#keywordintentclassifier" id="id22">KeywordIntentClassifier</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#selectors" id="id23">Selectors</a></p>
<ul>
<li><p><a class="reference internal" href="#response-selector" id="id24">Response Selector</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#tokenizers" id="id25">Tokenizers</a></p>
<ul>
<li><p><a class="reference internal" href="#whitespacetokenizer" id="id26">WhitespaceTokenizer</a></p></li>
<li><p><a class="reference internal" href="#jiebatokenizer" id="id27">JiebaTokenizer</a></p></li>
<li><p><a class="reference internal" href="#mitietokenizer" id="id28">MitieTokenizer</a></p></li>
<li><p><a class="reference internal" href="#spacytokenizer" id="id29">SpacyTokenizer</a></p></li>
<li><p><a class="reference internal" href="#converttokenizer" id="id30">ConveRTTokenizer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#entity-extractors" id="id31">Entity Extractors</a></p>
<ul>
<li><p><a class="reference internal" href="#mitieentityextractor" id="id32">MitieEntityExtractor</a></p></li>
<li><p><a class="reference internal" href="#spacyentityextractor" id="id33">SpacyEntityExtractor</a></p></li>
<li><p><a class="reference internal" href="#entitysynonymmapper" id="id34">EntitySynonymMapper</a></p></li>
<li><p><a class="reference internal" href="#crfentityextractor" id="id35">CRFEntityExtractor</a></p></li>
<li><p><a class="reference internal" href="#ducklinghttpextractor" id="id36">DucklingHTTPExtractor</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="word-vector-sources">
<h2><a class="toc-backref" href="#id9">Word Vector Sources</a><a class="headerlink" href="#word-vector-sources" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mitienlp">
<span id="id2"></span><h3><a class="toc-backref" href="#id10">MitieNLP</a><a class="headerlink" href="#mitienlp" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE initializer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Initializes mitie structures. Every mitie component relies on this,
hence this should be put at the beginning
of every pipeline that uses any mitie components.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>The MITIE library needs a language model file, that <strong>must</strong> be specified in
the configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieNLP&quot;</span>
  <span class="c1"># language model to load</span>
  <span class="nt">model</span><span class="p">:</span> <span class="s">&quot;data/total_word_feature_extractor.dat&quot;</span>
</pre></div>
</div>
<p>For more information where to get that file from, head over to
<a class="reference internal" href="../../user-guide/installation/#install-mitie"><span class="std std-ref">installing MITIE</span></a>.</p>
</dd>
</dl>
</div>
<div class="section" id="spacynlp">
<span id="id3"></span><h3><a class="toc-backref" href="#id11">SpacyNLP</a><a class="headerlink" href="#spacynlp" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>spacy language initializer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Initializes spacy structures. Every spacy component relies on this, hence this should be put at the beginning
of every pipeline that uses any spacy components.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>Language model, default will use the configured language.
If the spacy model to be used has a name that is different from the language tag (<code class="docutils literal notranslate"><span class="pre">&quot;en&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;de&quot;</span></code>, etc.),
the model name can be specified using this configuration variable. The name will be passed to <code class="docutils literal notranslate"><span class="pre">spacy.load(name)</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyNLP&quot;</span>
  <span class="c1"># language model to load</span>
  <span class="nt">model</span><span class="p">:</span> <span class="s">&quot;en_core_web_md&quot;</span>

  <span class="c1"># when retrieving word vectors, this will decide if the casing</span>
  <span class="c1"># of the word is relevant. E.g. `hello` and `Hello` will</span>
  <span class="c1"># retrieve the same vector, if set to `false`. For some</span>
  <span class="c1"># applications and models it makes sense to differentiate</span>
  <span class="c1"># between these two words, therefore setting this to `true`.</span>
  <span class="nt">case_sensitive</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="text-featurizers">
<h2><a class="toc-backref" href="#id12">Text Featurizers</a><a class="headerlink" href="#text-featurizers" title="Permalink to this headline">¶</a></h2>
<p>Text featurizers are divided into two different categories: sparse featurizers and dense featurizers.
Sparse featurizers are featurizers that return feature vectors with a lot of missing values, e.g. zeros.
As those feature vectors would normally take up a lot of memory, we store them as sparse features.
Sparse features only store the values that are non zero and their positions in the vector.
Thus, we save a lot of memroy and are able to train on larger datasets.</p>
<p>By default all featurizers will return a matrix of length (number-of-tokens x feature-dimension).
So, the returned matrix will have a feature vector for every token.
This allows us to train sequence models.
However, the additional token at the end (e.g. <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code>) contains features for the complete utterance.
This feature vector can be used in any non-sequence model.
The corresponding classifier can therefore decide what kind of features to use.</p>
<div class="section" id="mitiefeaturizer">
<h3><a class="toc-backref" href="#id13">MitieFeaturizer</a><a class="headerlink" href="#mitiefeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE intent featurizer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing, used as an input to intent classifiers that need intent features (e.g. <code class="docutils literal notranslate"><span class="pre">SklearnIntentClassifier</span></code>)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates feature for intent classification using the MITIE featurizer.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NOT used by the <code class="docutils literal notranslate"><span class="pre">MitieIntentClassifier</span></code> component. Currently, only <code class="docutils literal notranslate"><span class="pre">SklearnIntentClassifier</span></code> is able
to use precomputed features.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieFeaturizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="spacyfeaturizer">
<h3><a class="toc-backref" href="#id14">SpacyFeaturizer</a><a class="headerlink" href="#spacyfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>spacy intent featurizer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing, used as an input to intent classifiers that need intent features (e.g. <code class="docutils literal notranslate"><span class="pre">SklearnIntentClassifier</span></code>)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#spacynlp"><span class="std std-ref">SpacyNLP</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates feature for intent classification using the spacy featurizer.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyFeaturizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="convertfeaturizer">
<h3><a class="toc-backref" href="#id15">ConveRTFeaturizer</a><a class="headerlink" href="#convertfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates a vector representation of user message and response (if specified) using
<a class="reference external" href="https://github.com/PolyAI-LDN/polyai-models">ConveRT</a> model.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing, used as an input to intent classifiers and response selectors that need intent features and response
features respectively (e.g. <code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">ResponseSelector</span></code>)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#converttokenizer"><span class="std std-ref">ConveRTTokenizer</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for intent classification and response selection.
Uses the <a class="reference external" href="https://github.com/PolyAI-LDN/polyai-models#tfhub-signatures">default signature</a> to compute vector
representations of input text.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Since <code class="docutils literal notranslate"><span class="pre">ConveRT</span></code> model is trained only on an english corpus of conversations, this featurizer should only
be used if your training data is in english language.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">ConveRTFeaturizer</span></code> you need to install additional tensorflow libraries (<code class="docutils literal notranslate"><span class="pre">tensorflow_text</span></code> and
<code class="docutils literal notranslate"><span class="pre">tensorflow_hub</span></code>). You should do a pip install of Rasa with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">rasa[convert]</span></code> to install those.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;ConveRTFeaturizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="regexfeaturizer">
<h3><a class="toc-backref" href="#id16">RegexFeaturizer</a><a class="headerlink" href="#regexfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>regex feature creation to support intent and entity classification</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">text_features</span></code> and <code class="docutils literal notranslate"><span class="pre">tokens.pattern</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Sparse featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction and intent classification.
During training, the regex intent featurizer creates a list of <cite>regular expressions</cite> defined in the training
data format.
For each regex, a feature will be set marking whether this expression was found in the input, which will later
be fed into intent classifier / entity extractor to simplify classification (assuming the classifier has learned
during the training phase, that this set feature indicates a certain intent).
Regex features for entity extraction are currently only supported by the <code class="docutils literal notranslate"><span class="pre">CRFEntityExtractor</span></code> component!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There needs to be a tokenizer previous to this featurizer in the pipeline!</p>
</div>
</dd>
</dl>
</div>
<div class="section" id="countvectorsfeaturizer">
<h3><a class="toc-backref" href="#id17">CountVectorsFeaturizer</a><a class="headerlink" href="#countvectorsfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates bag-of-words representation of user message and label (intent and response) features</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing, used as an input to intent classifiers that
need bag-of-words representation of intent features
(e.g. <code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier</span></code>)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Sparse featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for intent classification and response selection.
Creates bag-of-words representation of user message and label features using
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">sklearn’s CountVectorizer</a>.
All tokens which consist only of digits (e.g. 123 and 99 but not a123d) will be assigned to the same feature.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the words in the model language cannot be split by whitespace,
a language-specific tokenizer is required in the pipeline before this component
(e.g. using <code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code> for Chinese).</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">sklearn’s CountVectorizer docs</a>
for detailed description of the configuration parameters.</p>
<p>This featurizer can be configured to use word or character n-grams, using <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> config parameter.
By default <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> is set to <code class="docutils literal notranslate"><span class="pre">word</span></code> so word token counts are used as features.
If you want to use character n-grams, set <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> to <code class="docutils literal notranslate"><span class="pre">char</span></code> or <code class="docutils literal notranslate"><span class="pre">char_wb</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Option ‘char_wb’ creates character n-grams only from text inside word boundaries;
n-grams at the edges of words are padded with space.
This option can be used to create <a class="reference external" href="https://arxiv.org/abs/1810.07150">Subword Semantic Hashing</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For character n-grams do not forget to increase <code class="docutils literal notranslate"><span class="pre">min_ngram</span></code> and <code class="docutils literal notranslate"><span class="pre">max_ngram</span></code> parameters.
Otherwise the vocabulary will contain only single letters</p>
</div>
<p>Handling Out-Of-Vacabulary (OOV) words:</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Enabled only if <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> is <code class="docutils literal notranslate"><span class="pre">word</span></code>.</p>
</div>
<p>Since the training is performed on limited vocabulary data, it cannot be guaranteed that during prediction
an algorithm will not encounter an unknown word (a word that were not seen during training).
In order to teach an algorithm how to treat unknown words, some words in training data can be substituted
by generic word <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code>.
In this case during prediction all unknown words will be treated as this generic word <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code>.</p>
<p>For example, one might create separate intent <code class="docutils literal notranslate"><span class="pre">outofscope</span></code> in the training data containing messages of
different number of <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> s and maybe some additional general words.
Then an algorithm will likely classify a message with unknown words as this intent <code class="docutils literal notranslate"><span class="pre">outofscope</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This featurizer creates a bag-of-words representation by <strong>counting</strong> words,
so the number of <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> in the sentence might be important.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> set a keyword for unseen words; if training data contains <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> as words in some
messages, during prediction the words that were not seen during training will be substituted with
provided <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code>; if <code class="docutils literal notranslate"><span class="pre">OOV_token=None</span></code> (default behaviour) words that were not seen during
training will be ignored during prediction time;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> set a list of words to be treated as <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> during training; if a list of words
that should be treated as Out-Of-Vacabulary is known, it can be set to <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> instead of manually
changing it in trainig data or using custom preprocessor.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Providing <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> is optional, training data can contain <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> input manually or by custom
additional preprocessor.
Unseen words will be substituted with <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> <strong>only</strong> if this token is present in the training
data or <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> list is provided.</p>
</div>
</div></blockquote>
<p>Sharing Vocabulary between user message and labels:</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Enabled only if <code class="docutils literal notranslate"><span class="pre">use_shared_vocab</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</div>
<p>Build a common vocabulary set between tokens in labels and user message.</p>
</div></blockquote>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;CountVectorsFeaturizer&quot;</span>
  <span class="c1"># whether to use a shared vocab</span>
  <span class="s">&quot;use_shared_vocab&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False,</span>
  <span class="c1"># whether to use word or character n-grams</span>
  <span class="c1"># &#39;char_wb&#39; creates character n-grams only inside word boundaries</span>
  <span class="c1"># n-grams at the edges of words are padded with space.</span>
  <span class="nt">analyzer</span><span class="p">:</span> <span class="s">&#39;word&#39;</span>  <span class="c1"># use &#39;char&#39; or &#39;char_wb&#39; for character</span>
  <span class="c1"># the parameters are taken from</span>
  <span class="c1"># sklearn&#39;s CountVectorizer</span>
  <span class="c1"># regular expression for tokens</span>
  <span class="nt">token_pattern</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">r&#39;(?u)\b\w\w+\b&#39;</span>
  <span class="c1"># remove accents during the preprocessing step</span>
  <span class="nt">strip_accents</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">None</span>  <span class="c1"># {&#39;ascii&#39;, &#39;unicode&#39;, None}</span>
  <span class="c1"># list of stop words</span>
  <span class="nt">stop_words</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">None</span>  <span class="c1"># string {&#39;english&#39;}, list, or None (default)</span>
  <span class="c1"># min document frequency of a word to add to vocabulary</span>
  <span class="c1"># float - the parameter represents a proportion of documents</span>
  <span class="c1"># integer - absolute counts</span>
  <span class="nt">min_df</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>  <span class="c1"># float in range [0.0, 1.0] or int</span>
  <span class="c1"># max document frequency of a word to add to vocabulary</span>
  <span class="c1"># float - the parameter represents a proportion of documents</span>
  <span class="c1"># integer - absolute counts</span>
  <span class="nt">max_df</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1.0</span>  <span class="c1"># float in range [0.0, 1.0] or int</span>
  <span class="c1"># set ngram range</span>
  <span class="nt">min_ngram</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>  <span class="c1"># int</span>
  <span class="nt">max_ngram</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>  <span class="c1"># int</span>
  <span class="c1"># limit vocabulary size</span>
  <span class="nt">max_features</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">None</span>  <span class="c1"># int or None</span>
  <span class="c1"># if convert all characters to lowercase</span>
  <span class="nt">lowercase</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>  <span class="c1"># bool</span>
  <span class="c1"># handling Out-Of-Vacabulary (OOV) words</span>
  <span class="c1"># will be converted to lowercase if lowercase is true</span>
  <span class="nt">OOV_token</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">None</span>  <span class="c1"># string or None</span>
  <span class="nt">OOV_words</span><span class="p">:</span> <span class="p p-Indicator">[]</span>  <span class="c1"># list of strings</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="intent-classifiers">
<h2><a class="toc-backref" href="#id18">Intent Classifiers</a><a class="headerlink" href="#intent-classifiers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mitieintentclassifier">
<h3><a class="toc-backref" href="#id19">MitieIntentClassifier</a><a class="headerlink" href="#mitieintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE intent classifier (using a
<a class="reference external" href="https://github.com/mit-nlp/MITIE/blob/master/examples/python/text_categorizer_pure_model.py">text categorizer</a>)</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>A tokenizer and a featurizer</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.98343</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This classifier uses MITIE to perform intent classification. The underlying classifier
is using a multi-class linear SVM with a sparse linear kernel (see
<a class="reference external" href="https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/text_categorizer_trainer.cpp#L222">MITIE trainer code</a>).</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieIntentClassifier&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="sklearnintentclassifier">
<h3><a class="toc-backref" href="#id20">SklearnIntentClassifier</a><a class="headerlink" href="#sklearnintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>sklearn intent classifier</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code> and <code class="docutils literal notranslate"><span class="pre">intent_ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>A featurizer</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.78343</span><span class="p">},</span>
    <span class="nt">&quot;intent_ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.1485910906220309</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;goodbye&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.08161531595656784</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;restaurant_search&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>The sklearn intent classifier trains a linear SVM which gets optimized using a grid search. In addition
to other classifiers it also provides rankings of the labels that did not “win”. The spacy intent classifier
needs to be preceded by a featurizer in the pipeline. This featurizer creates the features used for the
classification.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>During the training of the SVM a hyperparameter search is run to
find the best parameter set. In the config, you can specify the parameters
that will get tried</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SklearnIntentClassifier&quot;</span>
  <span class="c1"># Specifies the list of regularization values to</span>
  <span class="c1"># cross-validate over for C-SVM.</span>
  <span class="c1"># This is used with the ``kernel`` hyperparameter in GridSearchCV.</span>
  <span class="nt">C</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">1</span><span class="p p-Indicator">,</span> <span class="nv">2</span><span class="p p-Indicator">,</span> <span class="nv">5</span><span class="p p-Indicator">,</span> <span class="nv">10</span><span class="p p-Indicator">,</span> <span class="nv">20</span><span class="p p-Indicator">,</span> <span class="nv">100</span><span class="p p-Indicator">]</span>
  <span class="c1"># Specifies the kernel to use with C-SVM.</span>
  <span class="c1"># This is used with the ``C`` hyperparameter in GridSearchCV.</span>
  <span class="nt">kernels</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&quot;linear&quot;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="embeddingintentclassifier">
<h3><a class="toc-backref" href="#id21">EmbeddingIntentClassifier</a><a class="headerlink" href="#embeddingintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Embedding intent classifier</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code> and <code class="docutils literal notranslate"><span class="pre">intent_ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>A featurizer</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.8343</span><span class="p">},</span>
    <span class="nt">&quot;intent_ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.385910906220309</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;goodbye&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.28161531595656784</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;restaurant_search&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>The embedding intent classifier embeds user inputs and intent labels into the same space.
Supervised embeddings are trained by maximizing similarity between them.
This algorithm is based on <a class="reference external" href="https://arxiv.org/abs/1709.03856">StarSpace</a>.
However, in this implementation the loss function is slightly different and
additional hidden layers are added together with dropout.
This algorithm also provides similarity rankings of the labels that did not “win”.</p>
<p>The embedding intent classifier needs to be preceded by a featurizer in the pipeline.
This featurizer creates the features used for the embeddings.
It is recommended to use <code class="docutils literal notranslate"><span class="pre">CountVectorsFeaturizer</span></code> that can be optionally preceded
by <code class="docutils literal notranslate"><span class="pre">SpacyNLP</span></code> and <code class="docutils literal notranslate"><span class="pre">SpacyTokenizer</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If during prediction time a message contains <strong>only</strong> words unseen during training,
and no Out-Of-Vacabulary preprocessor was used,
empty intent <code class="docutils literal notranslate"><span class="pre">None</span></code> is predicted with confidence <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>The algorithm also has hyperparameters to control:</p>
<blockquote>
<div><ul>
<li><p>neural network’s architecture:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes_a</span></code> sets a list of hidden layer sizes before
the embedding layer for user inputs, the number of hidden layers
is equal to the length of the list</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes_b</span></code> sets a list of hidden layer sizes before
the embedding layer for intent labels, the number of hidden layers
is equal to the length of the list</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">share_hidden</span></code> if set to True, shares the hidden layers between user inputs and intent label</p></li>
</ul>
</div></blockquote>
</li>
<li><p>training:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> sets the number of training examples in one
forward/backward pass, the higher the batch size, the more
memory space you’ll need;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_strategy</span></code> sets the type of batching strategy,
it should be either <code class="docutils literal notranslate"><span class="pre">sequence</span></code> or <code class="docutils literal notranslate"><span class="pre">balanced</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code> sets the number of times the algorithm will see
training data, where one <code class="docutils literal notranslate"><span class="pre">epoch</span></code> equals one forward pass and
one backward pass of all the training examples;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_seed</span></code> if set to any int will get reproducible
training results for the same inputs;</p></li>
</ul>
</div></blockquote>
</li>
<li><p>embedding:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> sets the dimension of embedding space;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_neg</span></code> sets the number of incorrect intent labels,
the algorithm will minimize their similarity to the user
input during training;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">similarity_type</span></code> sets the type of the similarity,
it should be either <code class="docutils literal notranslate"><span class="pre">auto</span></code>, <code class="docutils literal notranslate"><span class="pre">cosine</span></code> or <code class="docutils literal notranslate"><span class="pre">inner</span></code>,
if <code class="docutils literal notranslate"><span class="pre">auto</span></code>, it will be set depending on <code class="docutils literal notranslate"><span class="pre">loss_type</span></code>,
<code class="docutils literal notranslate"><span class="pre">inner</span></code> for <code class="docutils literal notranslate"><span class="pre">softmax</span></code>, <code class="docutils literal notranslate"><span class="pre">cosine</span></code> for <code class="docutils literal notranslate"><span class="pre">margin</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_type</span></code> sets the type of the loss function,
it should be either <code class="docutils literal notranslate"><span class="pre">softmax</span></code> or <code class="docutils literal notranslate"><span class="pre">margin</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ranking_length</span></code> defines the number of top confidences over
which to normalize ranking results if <code class="docutils literal notranslate"><span class="pre">loss_type:</span> <span class="pre">&quot;softmax&quot;</span></code>;
to turn off normalization set it to 0</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mu_pos</span></code> controls how similar the algorithm should try
to make embedding vectors for correct intent labels,
used only if <code class="docutils literal notranslate"><span class="pre">loss_type</span></code> is set to <code class="docutils literal notranslate"><span class="pre">margin</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mu_neg</span></code> controls maximum negative similarity for
incorrect intents,
used only if <code class="docutils literal notranslate"><span class="pre">loss_type</span></code> is set to <code class="docutils literal notranslate"><span class="pre">margin</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_max_sim_neg</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> the algorithm only
minimizes maximum similarity over incorrect intent labels,
used only if <code class="docutils literal notranslate"><span class="pre">loss_type</span></code> is set to <code class="docutils literal notranslate"><span class="pre">margin</span></code>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scale_loss</span></code> if <code class="docutils literal notranslate"><span class="pre">true</span></code> the algorithm will downscale the loss
for examples where correct label is predicted with high confidence,
used only if <code class="docutils literal notranslate"><span class="pre">loss_type</span></code> is set to <code class="docutils literal notranslate"><span class="pre">softmax</span></code>;</p></li>
</ul>
</div></blockquote>
</li>
<li><p>regularization:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">C2</span></code> sets the scale of L2 regularization</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">C_emb</span></code> sets the scale of how important is to minimize
the maximum similarity between embeddings of different intent labels;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">droprate</span></code> sets the dropout rate, it should be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">droprate=0.1</span></code>
would drop out <code class="docutils literal notranslate"><span class="pre">10%</span></code> of input units;</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For <code class="docutils literal notranslate"><span class="pre">cosine</span></code> similarity <code class="docutils literal notranslate"><span class="pre">mu_pos</span></code> and <code class="docutils literal notranslate"><span class="pre">mu_neg</span></code> should be between <code class="docutils literal notranslate"><span class="pre">-1</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is an option to use linearly increasing batch size. The idea comes from
<a class="reference external" href="https://arxiv.org/abs/1711.00489">https://arxiv.org/abs/1711.00489</a>.
In order to do it pass a list to <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">[64,</span> <span class="pre">256]</span></code> (default behaviour).
If constant <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is required, pass an <code class="docutils literal notranslate"><span class="pre">int</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">64</span></code>.</p>
</div>
<p>In the config, you can specify these parameters.
The default values are defined in <code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier.defaults</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">defaults</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># nn architecture</span>
    <span class="c1"># sizes of hidden layers before the embedding layer for input words</span>
    <span class="c1"># the number of hidden layers is thus equal to the length of this list</span>
    <span class="s2">&quot;hidden_layers_sizes_a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="c1"># sizes of hidden layers before the embedding layer for intent labels</span>
    <span class="c1"># the number of hidden layers is thus equal to the length of this list</span>
    <span class="s2">&quot;hidden_layers_sizes_b&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="c1"># Whether to share the hidden layer weights between input words and labels</span>
    <span class="s2">&quot;share_hidden_layers&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="c1"># training parameters</span>
    <span class="c1"># initial and final batch sizes - batch size will be</span>
    <span class="c1"># linearly increased for each epoch</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
    <span class="c1"># how to create batches</span>
    <span class="s2">&quot;batch_strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;balanced&quot;</span><span class="p">,</span>  <span class="c1"># string &#39;sequence&#39; or &#39;balanced&#39;</span>
    <span class="c1"># number of epochs</span>
    <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="c1"># set random seed to any int to get reproducible results</span>
    <span class="s2">&quot;random_seed&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># embedding parameters</span>
    <span class="c1"># default dense dimension used if no dense features are present</span>
    <span class="s2">&quot;dense_dim&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="c1"># dimension size of embedding vectors</span>
    <span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="c1"># the type of the similarity</span>
    <span class="s2">&quot;num_neg&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="c1"># flag if minimize only maximum similarity over incorrect actions</span>
    <span class="s2">&quot;similarity_type&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>  <span class="c1"># string &#39;auto&#39; or &#39;cosine&#39; or &#39;inner&#39;</span>
    <span class="c1"># the type of the loss function</span>
    <span class="s2">&quot;loss_type&quot;</span><span class="p">:</span> <span class="s2">&quot;softmax&quot;</span><span class="p">,</span>  <span class="c1"># string &#39;softmax&#39; or &#39;margin&#39;</span>
    <span class="c1"># number of top intents to normalize scores for softmax loss_type</span>
    <span class="c1"># set to 0 to turn off normalization</span>
    <span class="s2">&quot;ranking_length&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="c1"># how similar the algorithm should try</span>
    <span class="c1"># to make embedding vectors for correct labels</span>
    <span class="s2">&quot;mu_pos&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># should be 0.0 &lt; ... &lt; 1.0 for &#39;cosine&#39;</span>
    <span class="c1"># maximum negative similarity for incorrect labels</span>
    <span class="s2">&quot;mu_neg&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span>  <span class="c1"># should be -1.0 &lt; ... &lt; 1.0 for &#39;cosine&#39;</span>
    <span class="c1"># flag: if true, only minimize the maximum similarity for incorrect labels</span>
    <span class="s2">&quot;use_max_sim_neg&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># scale loss inverse proportionally to confidence of correct prediction</span>
    <span class="s2">&quot;scale_loss&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># regularization parameters</span>
    <span class="c1"># the scale of L2 regularization</span>
    <span class="s2">&quot;C2&quot;</span><span class="p">:</span> <span class="mf">0.002</span><span class="p">,</span>
    <span class="c1"># the scale of how critical the algorithm should be of minimizing the</span>
    <span class="c1"># maximum similarity between embeddings of different labels</span>
    <span class="s2">&quot;C_emb&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="c1"># dropout rate for rnn</span>
    <span class="s2">&quot;droprate&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="c1"># visualization of accuracy</span>
    <span class="c1"># how often to calculate training accuracy</span>
    <span class="s2">&quot;evaluate_every_num_epochs&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>  <span class="c1"># small values may hurt performance</span>
    <span class="c1"># how many examples to use for calculation of training accuracy</span>
    <span class="s2">&quot;evaluate_on_num_examples&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># large values may hurt performance</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameter <code class="docutils literal notranslate"><span class="pre">mu_neg</span></code> is set to a negative value to mimic the original
starspace algorithm in the case <code class="docutils literal notranslate"><span class="pre">mu_neg</span> <span class="pre">=</span> <span class="pre">mu_pos</span></code> and <code class="docutils literal notranslate"><span class="pre">use_max_sim_neg</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference external" href="https://arxiv.org/abs/1709.03856">starspace paper</a> for details.</p>
</div>
</dd>
</dl>
</div>
<div class="section" id="keywordintentclassifier">
<span id="keyword-intent-classifier"></span><h3><a class="toc-backref" href="#id22">KeywordIntentClassifier</a><a class="headerlink" href="#keywordintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Simple keyword matching intent classifier, intended for small, short-term projects.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This classifier works by searching a message for keywords.
The matching is case sensitive by default and searches only for exact matches of the keyword-string in the user
message.
The keywords for an intent are the examples of that intent in the NLU training data.
This means the entire example is the keyword, not the individual words in the example.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This classifier is intended only for small projects or to get started. If
you have few NLU training data you can use one of our pipelines
<a class="reference internal" href="../choosing-a-pipeline/#choosing-a-pipeline"><span class="std std-ref">Choosing a Pipeline</span></a>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;KeywordIntentClassifier&quot;</span>
  <span class="nt">case_sensitive</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="selectors">
<h2><a class="toc-backref" href="#id23">Selectors</a><a class="headerlink" href="#selectors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="response-selector">
<span id="id4"></span><h3><a class="toc-backref" href="#id24">Response Selector</a><a class="headerlink" href="#response-selector" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Response Selector</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>A dictionary with key as <code class="docutils literal notranslate"><span class="pre">direct_response_intent</span></code> and value containing <code class="docutils literal notranslate"><span class="pre">response</span></code> and <code class="docutils literal notranslate"><span class="pre">ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>A featurizer</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the recommend python version to install?&quot;</span><span class="p">,</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.6485910906220309</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;faq&quot;</span><span class="p">},</span>
    <span class="nt">&quot;intent_ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.6485910906220309</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;faq&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.1416153159565678</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="nt">&quot;response_selector&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;faq&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;response&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.7356462617</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Supports 3.5, 3.6 and 3.7, recommended version is 3.6&quot;</span><span class="p">},</span>
        <span class="nt">&quot;ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.7356462617</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Supports 3.5, 3.6 and 3.7, recommended version is 3.6&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.2134543431</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;You can ask me about how to get started&quot;</span><span class="p">}</span>
        <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Response Selector component can be used to build a response retrieval model to directly predict a bot response from
a set of candidate responses. The prediction of this model is used by <a class="reference internal" href="../../core/retrieval-actions/#retrieval-actions"><span class="std std-ref">Retrieval Actions</span></a>.
It embeds user inputs and response labels into the same space and follows the exact same
neural network architecture and optimization as the <code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier</span></code>.</p>
<p>The response selector needs to be preceded by a featurizer in the pipeline.
This featurizer creates the features used for the embeddings.
It is recommended to use <code class="docutils literal notranslate"><span class="pre">CountVectorsFeaturizer</span></code> that can be optionally preceded
by <code class="docutils literal notranslate"><span class="pre">SpacyNLP</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If during prediction time a message contains <strong>only</strong> words unseen during training,
and no Out-Of-Vacabulary preprocessor was used,
empty response <code class="docutils literal notranslate"><span class="pre">None</span></code> is predicted with confidence <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>The algorithm includes all the hyperparameters that <code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier</span></code> uses.
In addition, the component can also be configured to train a response selector for a particular retrieval intent</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">retrieval_intent</span></code>: sets the name of the intent for which this response selector model is trained. Default <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
</ul>
</div></blockquote>
<p>In the config, you can specify these parameters.
The default values are defined in <code class="docutils literal notranslate"><span class="pre">ResponseSelector.defaults</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">defaults</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># nn architecture</span>
    <span class="c1"># sizes of hidden layers before the embedding layer for input words</span>
    <span class="c1"># the number of hidden layers is thus equal to the length of this list</span>
    <span class="s2">&quot;hidden_layers_sizes_a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="c1"># sizes of hidden layers before the embedding layer for intent labels</span>
    <span class="c1"># the number of hidden layers is thus equal to the length of this list</span>
    <span class="s2">&quot;hidden_layers_sizes_b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="c1"># Whether to share the hidden layer weights between input words and intent labels</span>
    <span class="s2">&quot;share_hidden_layers&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="c1"># training parameters</span>
    <span class="c1"># initial and final batch sizes - batch size will be</span>
    <span class="c1"># linearly increased for each epoch</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
    <span class="c1"># how to create batches</span>
    <span class="s2">&quot;batch_strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;balanced&quot;</span><span class="p">,</span>  <span class="c1"># string &#39;sequence&#39; or &#39;balanced&#39;</span>
    <span class="c1"># number of epochs</span>
    <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="c1"># set random seed to any int to get reproducible results</span>
    <span class="s2">&quot;random_seed&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># embedding parameters</span>
    <span class="c1"># default dense dimension used if no dense features are present</span>
    <span class="s2">&quot;dense_dim&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="c1"># dimension size of embedding vectors</span>
    <span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="c1"># the type of the similarity</span>
    <span class="s2">&quot;num_neg&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="c1"># flag if minimize only maximum similarity over incorrect actions</span>
    <span class="s2">&quot;similarity_type&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>  <span class="c1"># string &#39;auto&#39; or &#39;cosine&#39; or &#39;inner&#39;</span>
    <span class="c1"># the type of the loss function</span>
    <span class="s2">&quot;loss_type&quot;</span><span class="p">:</span> <span class="s2">&quot;softmax&quot;</span><span class="p">,</span>  <span class="c1"># string &#39;softmax&#39; or &#39;margin&#39;</span>
    <span class="c1"># number of top responses to normalize scores for softmax loss_type</span>
    <span class="c1"># set to 0 to turn off normalization</span>
    <span class="s2">&quot;ranking_length&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="c1"># how similar the algorithm should try</span>
    <span class="c1"># to make embedding vectors for correct intent labels</span>
    <span class="s2">&quot;mu_pos&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># should be 0.0 &lt; ... &lt; 1.0 for &#39;cosine&#39;</span>
    <span class="c1"># maximum negative similarity for incorrect intent labels</span>
    <span class="s2">&quot;mu_neg&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span>  <span class="c1"># should be -1.0 &lt; ... &lt; 1.0 for &#39;cosine&#39;</span>
    <span class="c1"># flag: if true, only minimize the maximum similarity for</span>
    <span class="c1"># incorrect intent labels</span>
    <span class="s2">&quot;use_max_sim_neg&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># scale loss inverse proportionally to confidence of correct prediction</span>
    <span class="s2">&quot;scale_loss&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># regularization parameters</span>
    <span class="c1"># the scale of L2 regularization</span>
    <span class="s2">&quot;C2&quot;</span><span class="p">:</span> <span class="mf">0.002</span><span class="p">,</span>
    <span class="c1"># the scale of how critical the algorithm should be of minimizing the</span>
    <span class="c1"># maximum similarity between embeddings of different intent labels</span>
    <span class="s2">&quot;C_emb&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="c1"># dropout rate for rnn</span>
    <span class="s2">&quot;droprate&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="c1"># visualization of accuracy</span>
    <span class="c1"># how often to calculate training accuracy</span>
    <span class="s2">&quot;evaluate_every_num_epochs&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>  <span class="c1"># small values may hurt performance</span>
    <span class="c1"># how many examples to use for calculation of training accuracy</span>
    <span class="s2">&quot;evaluate_on_num_examples&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># large values may hurt performance,</span>
    <span class="c1"># selector config</span>
    <span class="c1"># name of the intent for which this response selector is to be trained</span>
    <span class="s2">&quot;retrieval_intent&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="tokenizers">
<span id="id5"></span><h2><a class="toc-backref" href="#id25">Tokenizers</a><a class="headerlink" href="#tokenizers" title="Permalink to this headline">¶</a></h2>
<p>If you want to split intents into multiple labels, e.g. for predicting multiple intents or for
modeling hierarchical intent structure, use these flags with any tokenizer:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">intent_tokenization_flag</span></code> indicates whether to tokenize intent labels or not. By default this flag is set to
<code class="docutils literal notranslate"><span class="pre">False</span></code>, intent will not be tokenized.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">intent_split_symbol</span></code> sets the delimiter string to split the intent labels, default is underscore
(<code class="docutils literal notranslate"><span class="pre">_</span></code>).</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>All tokenizer add an additional token <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> to the end of the list of tokens when tokenizing
text and responses.</p>
</div>
</div></blockquote>
</li>
</ul>
<div class="section" id="whitespacetokenizer">
<h3><a class="toc-backref" href="#id26">WhitespaceTokenizer</a><a class="headerlink" href="#whitespacetokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using whitespaces as a separator</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates a token for every whitespace separated character sequence. Can be used to define tokens for the MITIE entity
extractor.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>Make the tokenizer not case sensitive by adding the <code class="docutils literal notranslate"><span class="pre">case_sensitive:</span> <span class="pre">false</span></code> option. Default being <code class="docutils literal notranslate"><span class="pre">case_sensitive:</span> <span class="pre">true</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;WhitespaceTokenizer&quot;</span>
  <span class="nt">case_sensitive</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="jiebatokenizer">
<h3><a class="toc-backref" href="#id27">JiebaTokenizer</a><a class="headerlink" href="#jiebatokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using Jieba for Chinese language</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the Jieba tokenizer specifically for Chinese
language. For language other than Chinese, Jieba will work as
<code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code>. Can be used to define tokens for the
MITIE entity extractor. Make sure to install Jieba, <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">jieba</span></code>.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>User’s custom dictionary files can be auto loaded by specific the files’ directory path via <code class="docutils literal notranslate"><span class="pre">dictionary_path</span></code></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;JiebaTokenizer&quot;</span>
  <span class="nt">dictionary_path</span><span class="p">:</span> <span class="s">&quot;path/to/custom/dictionary/dir&quot;</span>
</pre></div>
</div>
</dd>
</dl>
<p>If the <code class="docutils literal notranslate"><span class="pre">dictionary_path</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (the default), then no custom dictionary will be used.</p>
</div>
<div class="section" id="mitietokenizer">
<h3><a class="toc-backref" href="#id28">MitieTokenizer</a><a class="headerlink" href="#mitietokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using MITIE</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a></p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the MITIE tokenizer. Can be used to define
tokens for the MITIE entity extractor.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieTokenizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="spacytokenizer">
<h3><a class="toc-backref" href="#id29">SpacyTokenizer</a><a class="headerlink" href="#spacytokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list simple">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using spacy</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#spacynlp"><span class="std std-ref">SpacyNLP</span></a></p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the spacy tokenizer. Can be used to define
tokens for the MITIE entity extractor.</p>
</dd>
</dl>
</div>
<div class="section" id="converttokenizer">
<span id="id6"></span><h3><a class="toc-backref" href="#id30">ConveRTTokenizer</a><a class="headerlink" href="#converttokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list simple">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using ConveRT</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the ConveRT tokenizer. Must be used whenever the <code class="docutils literal notranslate"><span class="pre">ConveRTFeaturizer</span></code> is used.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="entity-extractors">
<h2><a class="toc-backref" href="#id31">Entity Extractors</a><a class="headerlink" href="#entity-extractors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mitieentityextractor">
<h3><a class="toc-backref" href="#id32">MitieEntityExtractor</a><a class="headerlink" href="#mitieentityextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE entity extraction (using a <a class="reference external" href="https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/ner_trainer.cpp">MITIE NER trainer</a>)</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>appends <code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a></p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;New York City&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
                  <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
                  <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
                  <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;MitieEntityExtractor&quot;</span><span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This uses the MITIE entity extraction to find entities in a message. The underlying classifier
is using a multi class linear SVM with a sparse linear kernel and custom features.
The MITIE component does not provide entity confidence values.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieEntityExtractor&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="spacyentityextractor">
<span id="id7"></span><h3><a class="toc-backref" href="#id33">SpacyEntityExtractor</a><a class="headerlink" href="#spacyentityextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>spaCy entity extraction</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>appends <code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#spacynlp"><span class="std std-ref">SpacyNLP</span></a></p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;New York City&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
                  <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
                  <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
                  <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;SpacyEntityExtractor&quot;</span><span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Using spaCy this component predicts the entities of a message. spacy uses a statistical BILOU transition model.
As of now, this component can only use the spacy builtin entity extraction models and can not be retrained.
This extractor does not provide any confidence scores.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>Configure which dimensions, i.e. entity types, the spacy component
should extract. A full list of available dimensions can be found in
the <a class="reference external" href="https://spacy.io/api/annotation#section-named-entities">spaCy documentation</a>.
Leaving the dimensions option unspecified will extract all available dimensions.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyEntityExtractor&quot;</span>
  <span class="c1"># dimensions to extract</span>
  <span class="nt">dimensions</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&quot;PERSON&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;LOC&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;ORG&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;PRODUCT&quot;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="entitysynonymmapper">
<h3><a class="toc-backref" href="#id34">EntitySynonymMapper</a><a class="headerlink" href="#entitysynonymmapper" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Maps synonymous entity values to the same value.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>modifies existing entities that previous entity extraction components found</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>If the training data contains defined synonyms (by using the <code class="docutils literal notranslate"><span class="pre">value</span></code> attribute on the entity examples).
this component will make sure that detected entity values will be mapped to the same value. For example,
if your training data contains the following examples:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[{</span>
  <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;I moved to New York City&quot;</span><span class="p">,</span>
  <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;inform_relocation&quot;</span><span class="p">,</span>
  <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;nyc&quot;</span><span class="p">,</span>
                <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span>
                <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
                <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
               <span class="p">}]</span>
<span class="p">},</span>
<span class="p">{</span>
  <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;I got a new flat in NYC.&quot;</span><span class="p">,</span>
  <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;inform_relocation&quot;</span><span class="p">,</span>
  <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;nyc&quot;</span><span class="p">,</span>
                <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
                <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">23</span><span class="p">,</span>
                <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
               <span class="p">}]</span>
<span class="p">}]</span>
</pre></div>
</div>
<p>This component will allow you to map the entities <code class="docutils literal notranslate"><span class="pre">New</span> <span class="pre">York</span> <span class="pre">City</span></code> and <code class="docutils literal notranslate"><span class="pre">NYC</span></code> to <code class="docutils literal notranslate"><span class="pre">nyc</span></code>. The entitiy
extraction will return <code class="docutils literal notranslate"><span class="pre">nyc</span></code> even though the message contains <code class="docutils literal notranslate"><span class="pre">NYC</span></code>. When this component changes an
exisiting entity, it appends itself to the processor list of this entity.</p>
</dd>
</dl>
</div>
<div class="section" id="crfentityextractor">
<h3><a class="toc-backref" href="#id35">CRFEntityExtractor</a><a class="headerlink" href="#crfentityextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>conditional random field entity extraction</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>appends <code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>A tokenizer</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;New York City&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
                  <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
                  <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.874</span><span class="p">,</span>
                  <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;CRFEntityExtractor&quot;</span><span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This component implements conditional random fields to do named entity recognition.
CRFs can be thought of as an undirected Markov chain where the time steps are words
and the states are entity classes. Features of the words (capitalisation, POS tagging,
etc.) give probabilities to certain entity classes, as are transitions between
neighbouring entity tags: the most likely set of tags is then calculated and returned.
If POS features are used (pos or pos2), spaCy has to be installed. If you want to use
additional features, such as pre-trained word embeddings, from any provided dense
featurizer, use <code class="docutils literal notranslate"><span class="pre">&quot;text_dense_features&quot;</span></code>.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;CRFEntityExtractor&quot;</span>
  <span class="c1"># The features are a ``[before, word, after]`` array with</span>
  <span class="c1"># before, word, after holding keys about which</span>
  <span class="c1"># features to use for each word, for example, ``&quot;title&quot;``</span>
  <span class="c1"># in array before will have the feature</span>
  <span class="c1"># &quot;is the preceding word in title case?&quot;.</span>
  <span class="c1"># Available features are:</span>
  <span class="c1"># ``low``, ``title``, ``suffix5``, ``suffix3``, ``suffix2``,</span>
  <span class="c1"># ``suffix1``, ``pos``, ``pos2``, ``prefix5``, ``prefix2``,</span>
  <span class="c1"># ``bias``, ``upper``, ``digit``, ``pattern``, and ``text_dense_features``</span>
  <span class="nt">features</span><span class="p">:</span> <span class="p p-Indicator">[[</span><span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;title&quot;</span><span class="p p-Indicator">],</span> <span class="p p-Indicator">[</span><span class="s">&quot;bias&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;suffix3&quot;</span><span class="p p-Indicator">],</span> <span class="p p-Indicator">[</span><span class="s">&quot;upper&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;pos&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;pos2&quot;</span><span class="p p-Indicator">]]</span>

  <span class="c1"># The flag determines whether to use BILOU tagging or not. BILOU</span>
  <span class="c1"># tagging is more rigorous however</span>
  <span class="c1"># requires more examples per entity. Rule of thumb: use only</span>
  <span class="c1"># if more than 100 examples per entity.</span>
  <span class="nt">BILOU_flag</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>

  <span class="c1"># This is the value given to sklearn_crfcuite.CRF tagger before training.</span>
  <span class="nt">max_iterations</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">50</span>

  <span class="c1"># This is the value given to sklearn_crfcuite.CRF tagger before training.</span>
  <span class="c1"># Specifies the L1 regularization coefficient.</span>
  <span class="nt">L1_c</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>

  <span class="c1"># This is the value given to sklearn_crfcuite.CRF tagger before training.</span>
  <span class="c1"># Specifies the L2 regularization coefficient.</span>
  <span class="nt">L2_c</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="ducklinghttpextractor">
<span id="id8"></span><h3><a class="toc-backref" href="#id36">DucklingHTTPExtractor</a><a class="headerlink" href="#ducklinghttpextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Duckling lets you extract common entities like dates,
amounts of money, distances, and others in a number of languages.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>appends <code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>nothing</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">53</span><span class="p">,</span>
                  <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;time&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span>
                  <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;2017-04-10T00:00:00.000+02:00&quot;</span><span class="p">,</span>
                  <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
                  <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;DucklingHTTPExtractor&quot;</span><span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>To use this component you need to run a duckling server. The easiest
option is to spin up a docker container using
<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">-p</span> <span class="pre">8000:8000</span> <span class="pre">rasa/duckling</span></code>.</p>
<p>Alternatively, you can <a class="reference external" href="https://github.com/facebook/duckling#quickstart">install duckling directly on your
machine</a> and start the server.</p>
<p>Duckling allows to recognize dates, numbers, distances and other structured entities
and normalizes them.
Please be aware that duckling tries to extract as many entity types as possible without
providing a ranking. For example, if you specify both <code class="docutils literal notranslate"><span class="pre">number</span></code> and <code class="docutils literal notranslate"><span class="pre">time</span></code> as dimensions
for the duckling component, the component will extract two entities: <code class="docutils literal notranslate"><span class="pre">10</span></code> as a number and
<code class="docutils literal notranslate"><span class="pre">in</span> <span class="pre">10</span> <span class="pre">minutes</span></code> as a time from the text <code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">there</span> <span class="pre">in</span> <span class="pre">10</span> <span class="pre">minutes</span></code>. In such a
situation, your application would have to decide which entity type is be the correct one.
The extractor will always return <cite>1.0</cite> as a confidence, as it is a rule
based system.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>Configure which dimensions, i.e. entity types, the duckling component
should extract. A full list of available dimensions can be found in
the <a class="reference external" href="https://duckling.wit.ai/">duckling documentation</a>.
Leaving the dimensions option unspecified will extract all available dimensions.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;DucklingHTTPExtractor&quot;</span>
  <span class="c1"># url of the running duckling server</span>
  <span class="nt">url</span><span class="p">:</span> <span class="s">&quot;http://localhost:8000&quot;</span>
  <span class="c1"># dimensions to extract</span>
  <span class="nt">dimensions</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&quot;time&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;number&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;amount-of-money&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;distance&quot;</span><span class="p p-Indicator">]</span>
  <span class="c1"># allows you to configure the locale, by default the language is</span>
  <span class="c1"># used</span>
  <span class="nt">locale</span><span class="p">:</span> <span class="s">&quot;de_DE&quot;</span>
  <span class="c1"># if not set the default timezone of Duckling is going to be used</span>
  <span class="c1"># needed to calculate dates from relative expressions like &quot;tomorrow&quot;</span>
  <span class="nt">timezone</span><span class="p">:</span> <span class="s">&quot;Europe/Berlin&quot;</span>
  <span class="c1"># Timeout for receiving response from http url of the running duckling server</span>
  <span class="c1"># if not set the default timeout of duckling http url is set to 3 seconds.</span>
  <span class="nt">timeout </span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
</div>


          </div>

          <div class="footer">
            <div class="questions">
              Stuck?
              <a class="reference external" href="https://forum.rasa.com" target="_blank">Ask a Question</a>
              or
              <a class="reference external" href="https://github.com/rasahq/rasa/issues" target="_blank">Create an Issue</a>
            </div>
            <div class="social">
              <a href="https://github.com/RasaHQ" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
              <a href="https://stackoverflow.com/search?q=rasa" target="_blank" title="Stack Overflow"><i class="fab fa-stack-overflow"></i></a>
              <a href="https://www.youtube.com/channel/UCJ0V6493mLvqdiVwOKWBODQ" target="_blank" title="YouTube"><i class="fab fa-youtube"></i></a>
              <a href="https://twitter.com/rasa_hq" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
            </div>

            <div class="copyright small">
            &copy;2020, Rasa Technologies | <a href="https://rasa.com/imprint/" target="_blank">Imprint</a> | <a href="https://rasa.com/privacy-policy/" target="_blank">Privacy Policy</a>
            
            </div>

          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>

    

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag(...arguments) {
      dataLayer.push(...arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-87333416-1', {
      'anonymize_ip': true,
    });
  </script>
  <script src="https://rasa.com/assets/js/js.cookies.js"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script type="opt-in" data-name="analytics" data-type="text/javascript" data-src="https://www.googletagmanager.com/gtag/js?id=UA-87333416-1"></script>
  <script type="opt-in" data-name="analytics" data-type="text/javascript" data-src="https://rasa.com/assets/js/userId.js"></script>

  <script type="text/javascript">
    var clipboard = new ClipboardJS('.copyable');
    clipboard.on('success', function(e) {
      gtag('event', e.action, {
        'event_category': 'code',
        'event_label': e.text
      });
      const id = e.text.replace(/ /g,'-');
      document.getElementById(id).classList.add('visible');
      setTimeout(function(){
        document.getElementById(id).classList.remove('visible');},
        800
      );
    });
    clipboard.on('error', function(e) {
      console.log(e);
    });

  </script>

  <!-- ACE Editor, Train & download buttons -->
  <script>
    let updateIntervalId;

    function uuidv4() {
      return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>
              (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)
      );
    }

    function fetchTracker(url, chatBlockId, conversationId) {
      $.ajax({
        url: url + "/conversations/" + conversationId + "/tracker",
        method: "get",
        dataType: 'json',
        contentType: 'application/json',
        success: function(tracker, status) {
          initChatBlock(url, chatBlockId, conversationId, tracker);
        }
      });
    }

    function sendMessage(url, chatBlockId, conversationId, tracker, message) {
      $.ajax({
        url: url + "/webhooks/rest/webhook",
        method: "post",
        dataType: 'json',
        contentType: 'application/json',
        data: JSON.stringify({
          sender: conversationId,
          message: message
        }),
        success: function(result, status) {
          fetchTracker(url, chatBlockId, conversationId);
        },
      });
    }

    function startFetchingTracker(url, chatBlockId, conversationId) {
      const interval = 2000;

      fetchTracker(url, chatBlockId, conversationId);

      updateIntervalId = setInterval(() => {
        fetchTracker(url, chatBlockId, conversationId);
      }, interval);
    }

    function initChatBlock(url, id, conversationId, tracker) {
      ChatBlock.default.init({
        onSendMessage: (message) => {
          sendMessage(url, id, conversationId, tracker, message);
        },
        username: conversationId,
        tracker: tracker,
        selector: id
      });
    }

    const chatBlockId = '#rasa-chat-block';
    const trackingId = uuidv4();

    $(document).ready(() => {
      // make editors work
      const editors = document.querySelectorAll('.ace-editor');

      const assignTrainingDataToButton = function(e, id, trainButton) {
        const trainingData = trainButton.data('training');

        trainButton.data('training', JSON.stringify({
          ...JSON.parse(trainingData || "{}") || {},
          [id]: e.getValue(),
        }));
      };

      ace.config.set('modePath', "/_static/ace/src-min-noconflict");

      Array.from(editors).forEach(function(editor) {
        const e = ace.edit(editor);
        const id = editor.dataset['id'];
        const trackingEndpoint = editor.dataset['tr-endpoint'];

        e.session.setMode("ace/mode/" + editor.dataset['language']);
        e.renderer.setShowGutter(false);
        e.renderer.setShowPrintMargin(false);
        e.renderer.setPadding(24);
        e.renderer.setScrollMargin(24);
        e.setHighlightActiveLine(false);

        if (trackingEndpoint) {
          e.on("change", function() {
            if (!editor.dataset.hasChanged) {
              editor.dataset.hasChanged = true;
              $.ajax({
                url: trackingEndpoint,
                method: "POST",
                data: {editor: id},
              });
            }
          });
        }

        const trainButton = $('.train__button');

        assignTrainingDataToButton(e, id, trainButton);
        e.on("blur", function() {
          assignTrainingDataToButton(e, id, trainButton);
        });

        editor.style.height = editor.dataset['height'] + 'px';
      });

      // initialize a chat block
      initChatBlock("", chatBlockId, trackingId, {});
    });

    // set train button callback
    $( ".train__button" ).click(function() {
      const trainButton = $('.train__button');
      const trainSpinner = $('.train__spinner');
      const downloadButton = $('.download__button');

      trainButton.prop("disabled", true);
      downloadButton.prop("disabled", true);
      trainSpinner.removeClass('train__spinner--hidden');

      if (updateIntervalId) {
        clearInterval(updateIntervalId);
        initChatBlock("", chatBlockId, trackingId, {});
      }

      $.ajax({
        url: trainButton.data('endpoint'),
        method: trainButton.data('method'),
        dataType: 'json',
        contentType: 'application/json',
        data: JSON.stringify({ tracking_id: trackingId, ...JSON.parse(trainButton.data('training')) }),
        success: function(result, status) {
          trainSpinner.addClass('train__spinner--hidden');
          trainButton.prop("disabled", false);

          if (result['project_download_url']) {
            trainSpinner.addClass('train__spinner--hidden');
            downloadButton.prop("disabled", false);
            downloadButton.data("url", result['project_download_url']);
          }

          if (result['rasa_service_url']) {
            const conversationId = uuidv4();
            startFetchingTracker(result['rasa_service_url'], chatBlockId, conversationId);
          }
        },
        error: function(result, status) {
          trainSpinner.addClass('train__spinner--hidden');
          trainButton.prop("disabled", false);
        }
      });
    });

    // set download button callback
    $( ".download__button" ).click(function() {
      const downloadButton = $('.download__button');
      const url = downloadButton.data("url");

      if (url) {
        location.href = url;
      }
    });
  </script>

  <!-- Dismissable announcement banner -->
  <script>
    var banners = document.querySelectorAll('.announcement-banner');

    Array.from(banners).forEach(function(banner) {
      var cookie_id = banner.dataset['cookie-id'];

      if (localStorage.getItem(cookie_id) !== 'true') {
        banner.classList.add('announcement-banner--visible');
      }

      var bannerCloseButton = banner.querySelector('.announcement-banner__close');

      bannerCloseButton && bannerCloseButton.addEventListener('click', function() {
        localStorage.setItem(cookie_id, 'true');
        banner.classList.remove('announcement-banner--visible');
      });
    });
  </script>

  <!-- onsite anchor fix (otherwise anchors scroll to far) -->
  <script>
    /* Adapted from https://stackoverflow.com/a/13067009/1906073 */
    (function(document, history, location) {
      var HISTORY_SUPPORT = !!(history && history.pushState);

      var anchorScrolls = {
        ANCHOR_REGEX: /^#[^ ]+$/,
        OFFSET_HEIGHT_PX: 66,

        /**
         * Establish events, and fix initial scroll position if a hash is provided.
         */
        init: function() {
          this.scrollToCurrent();
          $(window).on('hashchange', $.proxy(this, 'scrollToCurrent'));
          $('body').on('click', 'a', $.proxy(this, 'delegateAnchors'));
        },

        /**
         * Return the offset amount to deduct from the normal scroll position.
         * Modify as appropriate to allow for dynamic calculations
         */
        getFixedOffset: function() {
          return this.OFFSET_HEIGHT_PX;
        },

        /**
         * If the provided href is an anchor which resolves to an element on the
         * page, scroll to it.
         * @param  {String} href
         * @return {Boolean} - Was the href an anchor.
         */
        scrollIfAnchor: function(href, pushToHistory) {
          var match, anchorOffset;

          if(!this.ANCHOR_REGEX.test(href)) {
            return false;
          }

          match = document.getElementById(href.slice(1));

          if(match) {
            anchorOffset = $(match).offset().top - this.getFixedOffset();
            $('html, body').animate({ scrollTop: anchorOffset});

            // Add the state to history as-per normal anchor links
            if(HISTORY_SUPPORT && pushToHistory) {
              history.pushState({}, document.title, location.pathname + href);
            }
          }

          return !!match;
        },

        /**
         * Attempt to scroll to the current location's hash.
         */
        scrollToCurrent: function(e) {
          if(this.scrollIfAnchor(window.location.hash) && e) {
            e.preventDefault();
          }
        },

        /**
         * If the click event's target was an anchor, fix the scroll position.
         */
        delegateAnchors: function(e) {
          var elem = e.target;

          if(this.scrollIfAnchor(elem.getAttribute('href'), true)) {
            e.preventDefault();
          }
        }
      };

      $(document).ready($.proxy(anchorScrolls, 'init'));
    })(window.document, window.history, window.location);

  </script>
  <script type="opt-in" data-name="analytics" data-type="text/javascript" data-src="https://rasa.com/assets/js/u-info.js"></script>
      <div class="webchat-banner webchat-banner--hidden">
        <img src="https://rasa.com/assets/img/demo/sara_avatar.png" class="webchat-banner__avatar" alt="">
        👋 I can help you get started with Rasa and answer your technical questions.
        <button class="webchat-banner__close">
          <i class="fas fa-times"></i> 
        </button>
      </div>
      <div id="webchat">
        <script src="../../_static/rasa-webchat.js"></script>
        <script>
          WebChat.default.init({
            selector: "#webchat",
            initPayload: "/greet",
            socketUrl: "https://website-demo.rasa.com/",
            socketPath: "/socket.io",
            title: "Sara",
            subtitle: "I'm still in development",
            profileAvatar: "https://rasa.com/assets/img/demo/sara_avatar.png",
            showCloseButton: true,
            fullScreenMode: false,
            hideWhenNotConnected: false,
            connectOn: "open",
            autoClearCache: true,
            linksOpenTab: false,
            openLauncherImage: "../../_static/chat-icon.svg",
            customMessageDelay: (message) => {
              return 900 + message.length;
            },
            params: {
              storage: "local"
            },
            onWidgetEvent: {
              onChatOpen: () => {
                const webchatBanner = document.querySelector('.webchat-banner');
                if (webchatBanner) {
                  localStorage.setItem('WEBCHAT_BANNER_DISMISSED', 'true');
                  webchatBanner.classList.add('webchat-banner--hidden');
                }
              }
            }
          });

          try {
            const webchatBanner = document.querySelector('.webchat-banner');

            if (webchatBanner) {
              if (localStorage.getItem('WEBCHAT_BANNER_DISMISSED') !== 'true') {
                webchatBanner.classList.remove('webchat-banner--hidden');
              }

              const webChatBannerClose = document.querySelector('.webchat-banner__close');

              webChatBannerClose && webChatBannerClose.addEventListener('click', function() {
                localStorage.setItem('WEBCHAT_BANNER_DISMISSED', 'true');
                webchatBanner.classList.add('webchat-banner--hidden');
              });
            }
          } catch (e) {}
        </script>
      </div>
  </body>
</html>